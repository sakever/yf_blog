---
title: LLM 相关内容
date: 2025-11-11
sidebar: ture
categories:
  - 人工智能
tags:
  - LLM
---
## LLM 是什么
LLM 是近年来随着深度学习发展而出现的一种特定类型的 NLP 模型。这些模型非常大，拥有数亿甚至数千亿个参数

LLM 通过训练大规模的文本数据集（如互联网上的网页、书籍、文章等）来学习语言模式和结构。著名的 LLM 包括 Google 的 BERT、Facebook 的 RoBERTa、OpenAI 的 GPT 系列以及阿里云的 Qwen 等

这些模型能够执行多种 NLP 任务，如回答问题、生成文本、翻译等，并且由于其规模和复杂性，它们在许多任务上表现出优秀的性能

大型语言模型，通常是通过大规模无监督学习训练出来的，而不是传统的全监督学习。在无监督学习中，模型通过对大量文本数据进行自我学习，理解语言结构、语法和上下文关系，从而生成连贯、有意义的文本。具体来说，训练过程如下：

1，收集大量的文本数据，这些数据可以来自互联网、书籍、文章等各种来源
2，对收集到的数据进行清洗和格式化，去除无关信息，保留有用的文本内容。这个过程叫预处理
3，使用这些文本数据进行预训练，使其能够预测给定句子中的下一个词
4，微调（Fine-tuning）：虽然初始训练是无监督的，但在某些情况下，模型可以通过有监督的学习进一步优化，即利用带有标签的数据集进行微调，以提高特定任务的性能
### GPT 如何生成答案
ChatGPT 的所有输出都是即使生成出来的文本，文本生成的能力是它最基本的要求。这一项能力实际上是来自于它的训练方式，ChatGPT 在预训练时，是一个标准的**自回归语言模型任务**，这是 OpenAI 所有 GPT 系列模型的基底。所谓的自回归语言模型任务，通俗的理解是这样的：它可以根据已经输入的文本，它会根据概率预测接下来最可能出现的 token 

这里先说一下 token 是啥，GPT3.5有4096个 token 的限制，没有接触过 GPT 的同学可能会好奇为什么 GPT 不是4096个字节或者字符的限制，毕竟这些对于我们或者对于计算机而言都算是最小单位了。但是模型不这么认为，模型所使用的最小单位的字符片段就是 token，它可以是字（在中文里采用字是很常见的），也可以是词（英文的每个词天然地被空格隔开了，所以常采用词），甚至是字母。而上文所描述的 Embedding 技术就是原因（模型只能理解单词维度转化后的向量）

可以在这个网站上看到，一个句子大概多少 token：[token 计算器](https://app.linpp2009.com/zh/token-counter-visualizer) 

首先用户输入会被分词，转换成 token 序列每个 token 被映射为词向量（Embedding），并添加位置编码（Positional Encoding），如果用到自注意力机制还会有额外计算

然后模型会逐 token 生成，模型通过自回归生成输出，每次预测一个 token，直到终止条件。模型试图学习一个概率函数 ( P(w_t|w_1, w_2, ..., w_{t-1}) )，其中 ( w_t ) 是序列中的第 ( t ) 个词，( w_1, w_2, ..., w_{t-1} ) 是之前的词序列。这意味着模型在生成或预测每个词时，会考虑到前面所有词的信息

比如第一次预测，用户输入（"今天天气"），模型计算所有可能 token 的概率分布，按某种策略（如贪心搜索、采样）选择下一个 token。**然后将生成的 token 追加到输入序列**（"今天天气 好"）。再然后模型会重复预测，用**新序列继续预测下一个 token**（输入为"今天天气好"），直到达到终止条件
### NLP 发展的四个阶段
NLP 技术的发展可分为4个阶段/范式

1，全监督学习（非神经网络）。仅在目标任务的输入输出样本数据集上训练特定任务模型，其严重依赖特征工程
2，全监督学习（神经网络）。使得特征学习与模型训练相结合，于是研究重点转向了架构工程，即通过设计一个网络架构（如 CNN，RNN，Transformer）能够学习数据特征
3，Pre-train，Fine-tune。先在大数据集上预训练，再根据特定任务对模型进行微调，以适应于不同的下游任务。在这种范式下，研究重点转向了目标工程，设计在预训练和微调阶段使用的训练目标（损失函数）
4，Pre-train，Prompt，Predict。无需要 fine-tune，让预训练模型直接适应下游任务。方便省事，不需要每个任务每套参数，突破数据约束

上述内容基于论文：[Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing](https://arxiv.org/abs/2107.13586)

Fine-tuning 中：是预训练语言模型迁就各种下游任务。具体体现就是通过引入各种辅助任务，将其添加到预训练模型中，然后继续 pre-training，以便让其更加适配下游任务，这个过程中，预训练语言模型做出了更多的牺牲

Prompting 中，是各种下游任务迁就预训练语言模型。具体体现也是上面介绍的，我们需要对不同任务进行重构，使得它达到适配预训练语言模型的效果，这个过程中，是下游任务做出了更多的牺牲

这种方法的优点是给定一组合适的 prompt，以完全无监督的方式训练的单个 LLM 就能够用于解决大量任务。然而该方法也存在一个问题——这种方法引入了 prompt 挖掘工程的必要性，即需要找出最合适的 prompt 来让 LLM 解决面临的任务，即怎么做 Prompt Engineering

具体 Prompt 的做法是，将人为的规则给到预训练模型，使模型可以更好地理解人的指令的一项技术，以便更好地利用预训练模型。例如，在文本情感分类任务中，输入为 I love this movie，希望输出的是 positive/negative 中的一个标签。那么可以设置一个 Prompt，形如："The movie is ___"，然后让模型用来表示情感状态的答案（label），如 positive/negative，甚至更细粒度一些的 fantastic、boring 等，将空补全作为输出

## Transformer
Transformer 通过自注意力机制（Self-Attention Mechanism）解决了上述 RNN 的痛点。自注意力机制允许模型在处理每个位置的输入时，同时考虑整个序列的信息，而不仅仅是前一时间的信息
### 原理
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/bfd39df2cf9e480582929cfe7b2a6b08.png)
步骤如下，假如我们输入序列 The animal didn't cross the street because **it** was too tired。我们的问题是 it 这个词指代的是谁？。自注意力机制的目标就是帮助模型理解这个 it 到底指的是 animal 还是 street

1，我们先嵌入每个单词，让每个单词转换成高维向量，比如这里每个单词被转换成了300维度的向量
2，对每个单词赋予一个位置信息，表示这个单词在该句子中出现的位置
3，此时每个单词知道了自己的位置，但是还是不知道其他单词的位置，这时候我们需要生成 Q、K、V 矩阵：

- Query (Q)：代表当前位置的词想要查询或关注的信息
- Key (K)：**每个词的身份标识**，用于被 Query 匹配。相当于索引或标签，用来判断自己与 Query 的相关程度
- Value (V)：**实际要提取的信息**，如果 Query 与某个 Key 高度相关，那么对应的 Value 就会被提取出来，作为输出的主要成分

计算方式：Q = X · W_Q, K = X · W_K, V = X · W_V，W 是可学习权重矩阵，是之前已经算好的东西，这时候每个单词都生成了对应的 QKV 数据

4，计算注意力分数，分数 = softmax(Q·Kᵀ / √d_k)，也就是**归一化处理**，是当前词视角下这句话的全部上下文信息，然后用分数对 V 进行加权求和，得到最终包含整个句子上下文信息的数据，这个数据也是300维度的向量。以上就是注意力机制 attention 做的事情

这是自注意力机制的原理，还是使用上面的例子说明：

it 会生成 Query 向量，这个向量承载的问题是**我需要从其他所有词那里获取什么信息**？具体来说，它想问的是谁是我指代的对象？我的主语是什么？

其他所有词（如 The，animal，didn't, ...）都会生成自己的 Key 向量（我的身份或者说特征是动物、街道等）和 Value 向量（关于这个词的完整信息）

现在，我们让焦点词 it 的 Query 去和句子中所有词的 Key（包括它自己的Key）进行点积运算。点积的结果就是注意力分数。这个分数代表了当处理 it 这个词时，其他每个词的身份与 it 的问题的匹配程度。此时 it 的 Query（寻找主语）与 animal 的 Key（身份是动物）会产生一个很高的分数，而与其他词会生成一个很低的分数

Softmax 会将所有注意力分数通过 Softmax 函数进行归一化，**使得所有分数之和为1。这会得到一个注意力权重分布**。animal 将获得一个非常高的权重，我们假定它是0.9，street 将获得一个非常低的权重定为0.05，其他词分享剩余的微小权重

最后一步是我们用这些注意力权重，**对所有词的 Value 向量进行加权求和，生成 it 的最终输出向量**
```
“it”的最终输出向量 ≈ (0.9 * “animal”的Value) + (0.05 * “street”的Value) + ...
```
此时输出的向量包含了动物这个语义，因为在嵌入模型中离动物语义最为接近，此时我们可以确保模型理解了 it 这个词的含义，就是指动物。同时由于可以并行计算，其性能较高
### 多头注意力机制
注意，在生产环境中我们可能会用到**多头注意力机制**，多头指将输入的特征（KQV）通过多个独立的、并行运行的注意力模块进行处理，这些注意力模块就是头，每个头都会独立地计算注意力得分，并生成一个注意力加权后的输出。这些输出随后被合并以形成一个最终的表示

为鸡毛要这么做呢？因为一组注意力模型可能只能学到一种固定的语义关系。比如这个头注重动词和主语的关系，模型对这句话的理解就只注重动词和主语的关系了。**多组注意力模型让每组 KQV 关注不同的语义模式可以增强模型的表达能力**，比如头1关注主语动词，头2关注形容词名词，合起来就关注整句话了

还是举上面的例子来说明，假设我们使用了多头来分析，头1关注语法，头2关注语义

头1关注语法，它的主要任务是分析句法结构。it 的 Query：问的是谁是我的主语？

它发现 animal 的 Key 与这个问题最匹配。于是，它输出的向量主要包含了 animal 的语法角色信息

头2关注语义，它的主要任务是分析词义和逻辑。it 的 Query：问的是谁更符合疲惫这个状态

它从常识判断，动物会疲惫，而街道不会。因此，它也给 animal 很高的权重。它输出的向量主要包含了 animal 的语义信息（这是一个有生命的、会疲惫的实体）

Transformer 由编码器（Encoder，理解和处理输入序列，提取其中的特征和语义信息，Encoder 的自注意力是双向的，能看到整个输入）和解码器（Decoder，基于编码器的输出和已生成的内容，生成新的序列，Decoder 的自注意力是单向的，只能看左侧词，适合逐词生成） 组成，纯解码器模型如 GPT 只用解码器部分，**GPT 在训练的时候在和用户交互的时候都用到了注意力机制**，只用 Decoder 的原因是因为 GPT 的任务是生成文本，只需要根据上文生成下文，所以不需要 Encoder。而且 T5 这样的用于处理输入输出任务的模型，则用到了 Encoder 和 Decoder，适合处理一些翻译、摘要任务

注意力机制可以同时作用于用户输入的 prompt （在编码阶段，模型使用自注意力处理输入文本，理解其含义和关系）和模型生成的内容（在解码阶段，模型一边生成文本，一边使用自注意力关注已生成的内容，并通过交叉注意力关注输入内容）

具体实现为，**所有注意力头共享同一套 Key 和 Value，但每个头有独立的 Query**

其实我们还有 Grouped-Query Attention，是 MQA 的改进版，处理了 MQA 的精度损失有时过大的问题
### 主流架构
目前大模型有两种主流架构，分别是编码器+解码器，以及仅解码器

编码器+解码器架构的代表模型为 T5、BART、mT5。比较适合翻译、摘要、问答等任务，需要的参数量较大（两个组件），其训练目标主要是为了去噪和重构。而解码器代表的模型为 GPT、LLaMA、PaLM，生成时只能看前面的词。比较适合文本生成、对话、续写等任务参数量相对较小（一个组件），其训练的重点放在下一个词预测上

目前大模型大部分是 Decoder only 结构，因为 decoder-only 结构模型在没有任何微调数据的情况下，zero-shot 的表现能力最好。而 encoder-decoder 则 需要在一定量的标注数据上做 multitask-finetuning 才能够激发最佳性能
### 涌现能力
随着模型参数的增加，模型会自然而然的获得一些意料之外的能力

涌现能力 Emergent Ability 是指大型语言模型在参数规模达到某个临界点后，突然表现出的新能力，这些能力在小模型中不存在或表现极差。比如思维链、上下文学习等等

这个原因是比如我们假设某个任务 T 有 5 个子任务 Sub-T 构成，每个 sub-T 随着模型增长，指标从 40% 提升到 60%，但是最终任务的指标只从 1.1% 提升到了 7%，也就是整整6倍多，也就是说宏观上看到 了涌现现象，但是子任务效果其实是平滑增长的。说诉一点就叫力大砖飞

## 总结
如果想训练一个语言模型应该这么做：

1，准备数据与预训练：让模型学会美食评论的基本语言模式和知识（如食物名称、形容词、评价逻辑）

数据：你收集了10万条美食评论，如：“这家店的牛肉面汤头浓郁，面条劲道。”，然后进行分词嵌入，将句子切成模型认识的Token：[“这家”, “店的”, “牛肉面”, “汤头”, “浓郁”, “，”, “面条”, “劲道”, “。”]。用自监督的方式，让模型做做预测。取前 N 个词作为输入，让模型预测第 N+1 个词。

- 输入：“这家店的牛肉面汤头浓郁，”
- 标签：“面条”

2，模型架构与训练（核心），使用神经网络、损失函数和 Transformer（或其变体，这是实现强大语言理解能力的核心技术），这是现代 LLM 的唯一路径

训练过程：输入句子被转换为词向量（每个词变成一个数字向量）并加上位置编码（让模型知道词的顺序），这些向量进入 Transformer 的编码器堆栈。
当模型处理“浓郁”这个词时，自注意力机制会计算它与句子中所有其他词（“这家”、“店”、“牛肉面”…）的关联度，从而理解“浓郁”是在形容“汤头”

经过多层变换后，模型对最后一个词（“，”）位置形成了包含整个上文语境的最终向量表示，这个向量通过一个线性层 + Softmax，输出一个在整个词表上的概率分布

理想情况下，“面条”的概率应该最高。“米饭”、“包子”可能有较低概率。完全不相关的词如“跑步”概率接近零

我们还会使用到损失函数，它比较模型预测的概率分布（“面条”概率0.8，“米饭”概率0.1…）与真实标签（“面条”概率1.0）的差异。目标就是最小化这个损失，让模型的预测越来越准

3，使用模型（推理）：训练完成后，你可以让模型续写评论。输入：“我尝试了他们的招牌披萨，”

模型运行时就是同样的流程：分词、向量化、通过 Transformer 处理。自注意力机制再次工作，处理“披萨”时，它会关联到“招牌”和“尝试”，理解这是一个被尝试的招牌菜

模型输出下一个词的概率分布，假设最高的是“芝士”。模型会把“芝士”加到输入后面，变成：“我尝试了他们的招牌披萨，芝士”，再次输入模型，预测下一个词（可能是“非常”或“拉丝”）

如此循环往复（自回归生成），就能生成完整句子：“我尝试了他们的招牌披萨，芝士非常丰富，饼底酥脆。”
## Agent 所用技术
AI agent 本质上是一个构建在 LLM 之上的智能应用，也就是说 AI agent 是大模型的上层应用。如果说把 AI 比做一个人，那么大模型就是这个人的大脑，虽然它拥有了智能，但其却没有能够真正做事的实体。而 AI agent 就相当于人的手脚眼睛和嘴巴，以及各种人类能够利用的工具

简单来说，我们想给 LLM 配置上与物理世界互动的工具、记忆能力、规划思考能力，举个例子是：给他一个问题，让他在不知道某个单词是什么的情况下，自己去百度，然后再将调查结果返回给我们。即 LLM 将提供单纯的思考能力，而 Agent 会提供 LLM（接受输入、思考、输出）+ 记忆 + 工具 + 规划能力

目前机构在使用大模型应用的过程中，一般使用到以下技术，从易到难分别为：提示工程、RAG（检索增强生成）、精调、预训练，一般来说这四种技术不会单独使用，在不同场景下，会将四种技术搭配使用
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/b7888497d8a64fc589ad6d40cf7b7dd0.png)
### 提示工程
提示工程指针对性的设计提示词，来引导大模型产生特点场景的输出。提示工程适用于刚接触大模型的企业新手，采用这种方式能以最小资源投入、快速探索应用。当然局限性也很明显，若大模型本身内含的行业数据较少，效果就会比较差。在现实使用的过程中，我们往往建立一个中台的项目，去人工筛选提示词或者配置固定提示词，将一个通用领域的大模型引导至专业领域。这里使用的技术就是零样本提示，非常的简单方便易于实现

零样本提示或者少样本提示，都是利用了模型那优秀的学习能力去实现的，假设我们希望模型学会识别文本情感（正面或负面）。在少样本提示中，我们可以提供几个带标签的情感分析示例：

```
下面是一些文本及其情感标签：
- “这家餐厅的食物非常美味！”：正面
- “我对这部电影感到非常失望。”：负面
- “这本书写得真好。”：正面
- “我不喜欢这个手机的设计。”：负面
现在请判断这段文本的情感：“这次旅行真是令人愉快的经历。”
```
模型会根据提供的示例，学习到正面和负面情感的表达方式，然后尝试对新的文本进行情感分类。在大模型客服业务中，我们的提示词可能是如果用户问了什么，则回答什么

当然提示工程远不止这些，COT（思维链和思维树）则是属于比较高端的技术，样本提示利用模型的举一反三能力，而思维链则是先思考如何拆解这个问题，然后再一步步处理这个问题。即模型收到问题后，会进行下面的处理：

1，模型生成一系列中间思考步骤，这些步骤反映了模型如何利用已知信息进行推理的过程。例如，在解决数学问题时，中间步骤可能包括列出公式、代入数值、逐步计算等
2，整合中间步骤：模型将中间思考步骤整合起来，形成一条连贯的思考链路，这有助于模型更好地理解和推导最终答案
3，生成最终答案：基于中间思考步骤，模型生成最终的答案。这个过程可能需要多次迭代和修正，直到找到最合理的答案为止

COT 的难点是通过设计特定的提示（Prompt），引导模型按照预设的格式生成中间思考步骤。**提示可以是模板化的，也可以是基于问题内容动态生成的**
### RAG（检索增强生成）
RAG 指在不改变大模型本身的基础上，通过外挂知识库等方式，为模型提供特定领域的数据信息输入。他是一种**结合信息检索（Retrieval） 和文本生成（Generation） 的技术**，旨在让大语言模型（LLM）生成更准确、实时的回答。这里的具体应用是当用户提出一个问题时，程序会去向量库中首先检索相关数据，找到与问题相关的信息，然后将检索出来的内容作为上下文一起传给大模型，让大模型了解相关知识以了解更加精准的内容。这么说可能有些抽象，举个实际一点的例子吧：

比如用户发问：我想了解一下 mybatis-plus 的使用。但是这个时候假如模型没有 mybatis-plus 相关训练数据，他不知道 mp 相关知识，因此根本没有办法返回用户正确信息，如果使用微调的方式处理这个问题，我们可能需要两三天，还需要大量数据，更重要的问题是，这么做很烧钱

此时我们先将用户的问题去向量库（比如 es）中搜索这个问题，假设向量库提前存好了全量的 mp 信息，向量库提取了关键字 mp，并且返回了全套的 mp 使用文档。我们在把这个文档当作 system 设置去访问大模型，这个时候大模型就知道要给用户返回什么了

上面图中的 RAG 模块所使用到的技术，其实都不是 LLM 专属的，是向量库中所使用到的技术。embedding 是文本向量化技术，上文有介绍

DPR，全称为 Dense Passage Retrieval，是一种用于开放域问答系统中的密集表示检索技术，它引入了深度学习技术来优化文档检索过程，旨在解决传统信息检索技术在大规模文本数据上效率和效果不佳的问题

重排序是在检索增强生成模型中一个关键的后续处理步骤，用于优化从文档库中检索到的相关段落或文档的顺序。在 RAG 模型中，通常先使用一个检索器从大量文档中找出与查询最相关的内容，然后这些内容传递给生成模型来进行排序。因为最初的检索结果可能并不完美，重排序步骤旨在进一步提升检索质量，确保最相关的信息位于首位

重排序通常涉及以下步骤，比如用户需要搜索一个有关 ssm 的知识：

1，初步检索：使用快速检索算法（如 BM25）找到与查询相关的初始候选集合
2，深度学习模型评分：将初步检索结果中的每个文档或段落以及原始查询输入到深度学习模型（如 BERT 或其他 Transformer 模型）中，该模型会为每个文档分配一个相关性得分
3，排序：根据深度学习模型给出的得分，重新排列初步检索结果的顺序，确保得分最高的文档排在前面

目前业内做 RAG 的通用方式是，上传一个大文档，然后将这个文档切分成多个小知识片段，这些小知识片段都包含了一问一答，来实现知识库的。这么做可以通过将文档分解成更小的单元，可以更精确地匹配用户的查询，减少无关信息的干扰，从而提高检索的准确性和效率，同时当需要更新或修正某部分信息时，只需修改相应的片段，而不需要重新处理整个文档，这使得知识库的维护变得更加灵活和高效。而存入大文档，就没有这些优点了
### 预训练
预训练（Pretraining）是机器学习，特别是深度学习中的一种重要技术，初代的 GPT 模型采用的就已经是生成式的预训练（这也是 GPT 名字的由来，Generative Pre-Training，即生成式预训练）

在预训练阶段，模型（如 Transformer 架构的 BERT、GPT 或 T5 等）会在一个非常大的文本数据集上进行训练，这个数据集通常是互联网抓取的网页、书籍、新闻等。模型的目标是学习语言的一般规律和模式，而不是针对特定任务。这个过程通常使用无监督学习任务，如自动生成下一个单词（自回归预训练）或填充被遮蔽的单词（掩码语言模型预训练）。大模型的本质，是基于大样本训练出的预测模型。模型基于给它输入的语料，根据概率预测可能的回答。模型的预测结果受语料限制

举例说一些模型技术：

1，掩码语言模型（Masked Language Model, MLM）：随机掩盖输入文本中的一部分单词，让模型预测这些被掩盖的单词。这有助于模型学习上下文信息
2，下一句预测（Next Sentence Prediction, NSP）：给模型两个句子，让它判断第二个句子是否是第一个句子的下一句。这有助于模型理解句子之间的关系

### 微调
预训练完成后一般会进入微调（也叫精调）阶段，预训练在大规模未标注数据集上先进行学习，然后将学到的知识转移到特定任务的微调阶段，一般通过人工、程序标注等方式，来处理大模型中不符合预期的返回

预训练完成后，模型已经具备了一定的语言理解能力。在微调阶段，模型会针对特定的下游任务（如文本分类、问答系统、机器翻译等）进行进一步的训练，这个阶段叫 Fine-tuning。这时，我们会用到带有标签的小型数据集，调整模型的参数以优化特定任务的性能。具体来说，是通过人类的反馈来约束模型，从而让模型回答出人类满意的答案。当预测模型执行某个任务时，人类可以提供正面或负面的反馈（手动去选择对错），以指导模型的行为

预训练的好处在于，它允许模型在大量数据上学习通用的语言表示，这些表示可以捕捉到语言的丰富结构和语义信息。这样，在微调时，模型只需要少量的标记数据就能达到较好的性能，尤其对于那些标记数据稀缺的任务来说，预训练极大地提高了效率和效果

**微调（Fine-tuning）的本质是监督学习**（Supervised Learning），用标注数据调整模型参数，使其更适配特定任务。我们喂给模型的数据数据是输入输出配对样本。在预训练模型的基础上，用新数据继续训练（通常损失函数是交叉熵）。让 LLM 学会按指令生成答案，而不是单纯补全文本

主流方法为 LoRA，仅训练低秩矩阵。如果想自己微调模型，可以参考  [ollama 使用自己的微调模型](https://blog.csdn.net/spiderwower/article/details/138755776)
